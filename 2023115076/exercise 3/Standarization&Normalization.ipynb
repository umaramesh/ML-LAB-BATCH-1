{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT-95RDeFzsI"
      },
      "source": [
        "# **Preprocessing Steps**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DA0F6uhF3bz"
      },
      "source": [
        "Binning Algorithms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0SGOeKtEzrp",
        "outputId": "4dcb118a-5361-4e46-a399-1d06d8eb3c49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Equal-Width Bins: [[4, 8, 9], [15, 21, 21], [24, 25, 26, 28, 29, 34]]\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "data = [4, 8, 9, 15, 21, 21, 24, 25, 26, 28, 29, 34]\n",
        "\n",
        "def equal_width_binning(data, num_bins):\n",
        "    min_val = min(data)\n",
        "    max_val = max(data)\n",
        "    bin_width = (max_val - min_val) / num_bins\n",
        "\n",
        "    bins = [[] for _ in range(num_bins)]\n",
        "\n",
        "    for value in data:\n",
        "        bin_index = int((value - min_val) / bin_width)\n",
        "        # Place max value in the last bin\n",
        "        if bin_index >= num_bins:\n",
        "            bin_index = num_bins - 1\n",
        "        bins[bin_index].append(value)\n",
        "\n",
        "    return bins\n",
        "\n",
        "# Example Usage\n",
        "binned_data = equal_width_binning(data, 3)\n",
        "print(f\"Equal-Width Bins: {binned_data}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAPjRVncGDSo"
      },
      "source": [
        "Min-Max Normalization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93v7IApYGHHH",
        "outputId": "1f5ad2b7-4546-4ad6-8319-63cb92265280"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Min-Max Normalized Data: [0.0, 0.125, 0.25, 0.5, 1.0]\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "data = [200, 300, 400, 600, 1000]\n",
        "\n",
        "def min_max_normalize(data):\n",
        "    min_val = min(data)\n",
        "    max_val = max(data)\n",
        "    data_range = max_val - min_val\n",
        "\n",
        "    if data_range == 0:\n",
        "        return [0.5 for _ in data] # Or handle as an error\n",
        "\n",
        "    normalized_data = []\n",
        "    for value in data:\n",
        "        normalized_value = (value - min_val) / data_range\n",
        "        normalized_data.append(normalized_value)\n",
        "\n",
        "    return normalized_data\n",
        "\n",
        "# Example Usage\n",
        "normalized = min_max_normalize(data)\n",
        "print(f\"Min-Max Normalized Data: {normalized}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qquxLdrFGKP2"
      },
      "source": [
        "Hypothesis Testing (Student's t-test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8Asvh_uGK2t",
        "outputId": "232aa70b-5076-47b9-8d14-564d1bcdb714"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "T-statistic: 1.337953738763802\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "sample1 = [21.5, 24.5, 23.6, 28.9, 25.1]\n",
        "sample2 = [20.1, 22.5, 21.7, 25.4, 23.8]\n",
        "\n",
        "def mean(data):\n",
        "    return sum(data) / len(data)\n",
        "\n",
        "def std_dev(data):\n",
        "    n = len(data)\n",
        "    if n < 2:\n",
        "        return 0\n",
        "    mu = mean(data)\n",
        "    variance = sum([(x - mu) ** 2 for x in data]) / (n - 1)\n",
        "    return variance ** 0.5\n",
        "\n",
        "def independent_t_test(sample1, sample2):\n",
        "    n1, n2 = len(sample1), len(sample2)\n",
        "    mean1, mean2 = mean(sample1), mean(sample2)\n",
        "    std1, std2 = std_dev(sample1), std_dev(sample2)\n",
        "\n",
        "    # Pooled standard deviation\n",
        "    sp = (((n1 - 1) * std1**2 + (n2 - 1) * std2**2) / (n1 + n2 - 2)) ** 0.5\n",
        "\n",
        "    # t-statistic\n",
        "    t_stat = (mean1 - mean2) / (sp * ((1/n1) + (1/n2))**0.5)\n",
        "    return t_stat\n",
        "\n",
        "# Example Usage\n",
        "t_statistic = independent_t_test(sample1, sample2)\n",
        "print(f\"T-statistic: {t_statistic}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMoz3w1kGOMC"
      },
      "source": [
        "Chi-Square Test (for Independence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9tvrkidGOuV",
        "outputId": "9fca3a4f-e9de-4c8d-8073-0374a49e5185"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chi-Square Statistic: 11.666666666666668\n"
          ]
        }
      ],
      "source": [
        "# Sample Data: Observed frequencies in a contingency table\n",
        "# Format: [[row1_col1, row1_col2], [row2_col1, row2_col2]]\n",
        "contingency_table = [[50, 20], [30, 40]] # Example: Smoker vs. Non-smoker and Lung Disease vs. No Disease\n",
        "\n",
        "def chi_square_test(table):\n",
        "    num_rows = len(table)\n",
        "    num_cols = len(table[0])\n",
        "\n",
        "    # Calculate totals\n",
        "    row_totals = [sum(row) for row in table]\n",
        "    col_totals = [sum(row[i] for row in table) for i in range(num_cols)]\n",
        "    grand_total = sum(row_totals)\n",
        "\n",
        "    chi_square_stat = 0\n",
        "\n",
        "    for i in range(num_rows):\n",
        "        for j in range(num_cols):\n",
        "            # Calculate expected frequency\n",
        "            expected = (row_totals[i] * col_totals[j]) / grand_total\n",
        "            observed = table[i][j]\n",
        "            chi_square_stat += ((observed - expected) ** 2) / expected\n",
        "\n",
        "    return chi_square_stat\n",
        "\n",
        "# Example Usage\n",
        "chi_square_value = chi_square_test(contingency_table)\n",
        "print(f\"Chi-Square Statistic: {chi_square_value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiYLs5XaGQfK"
      },
      "source": [
        "Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeNwtpvtGTft",
        "outputId": "1d231b83-a00e-452b-c793-c6d8effc624e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            "        Pred_Pos  Pred_Neg\n",
            "Act_Pos [   4    ] [   1    ]\n",
            "Act_Neg [   2    ] [   3    ]\n",
            "\n",
            "Metrics:\n",
            "Accuracy: 0.7000\n",
            "Precision: 0.6667\n",
            "Recall: 0.8000\n",
            "F1 Score: 0.7273\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n",
        "y_pred = [1, 1, 1, 0, 0, 1, 0, 1, 1, 0]\n",
        "\n",
        "def confusion_matrix_metrics(y_true, y_pred):\n",
        "    tp, tn, fp, fn = 0, 0, 0, 0\n",
        "\n",
        "    for actual, predicted in zip(y_true, y_pred):\n",
        "        if actual == 1 and predicted == 1:\n",
        "            tp += 1\n",
        "        elif actual == 0 and predicted == 0:\n",
        "            tn += 1\n",
        "        elif actual == 0 and predicted == 1:\n",
        "            fp += 1\n",
        "        elif actual == 1 and predicted == 0:\n",
        "            fn += 1\n",
        "\n",
        "    # Metrics\n",
        "    accuracy = (tp + tn) / len(y_true) if len(y_true) > 0 else 0\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(f\"        Pred_Pos  Pred_Neg\")\n",
        "    print(f\"Act_Pos [ {tp:^6} ] [ {fn:^6} ]\")\n",
        "    print(f\"Act_Neg [ {fp:^6} ] [ {tn:^6} ]\")\n",
        "    print(\"\\nMetrics:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1_score:.4f}\")\n",
        "\n",
        "# Example Usage\n",
        "confusion_matrix_metrics(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySoQkKoKGcAn"
      },
      "source": [
        "Implement Dimensionality reduction using Principle component Analysis method on a dataset iris\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xv1o7CHsGc4s",
        "outputId": "003579d4-a891-4684-f30d-daa0d7c198e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original number of features: 4\n",
            "Reduced number of features: 2\n",
            "\n",
            "First 5 rows of the new 2D dataset:\n",
            "[-2.3455, 1.1317]\n",
            "[-1.9404, -1.0635]\n",
            "[-2.2902, -0.2743]\n",
            "[0.7965, 0.6176]\n",
            "[0.4633, 0.4336]\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "# --- 1. The Iris Dataset ---\n",
        "# Features: [sepal_length, sepal_width, petal_length, petal_width]\n",
        "# Using a subset for clarity\n",
        "X = [\n",
        "    [5.1, 3.5, 1.4, 0.2], [4.9, 3.0, 1.4, 0.2], [4.7, 3.2, 1.3, 0.2],\n",
        "    [7.0, 3.2, 4.7, 1.4], [6.4, 3.2, 4.5, 1.5], [6.9, 3.1, 4.9, 1.5],\n",
        "    [6.3, 3.3, 6.0, 2.5], [5.8, 2.7, 5.1, 1.9], [7.1, 3.0, 5.9, 2.1]\n",
        "]\n",
        "# Labels for coloring the plot later (0=Setosa, 1=Versicolor, 2=Virginica)\n",
        "y = [0, 0, 0, 1, 1, 1, 2, 2, 2]\n",
        "\n",
        "\n",
        "# --- 2. Helper Functions (No external libraries for math) ---\n",
        "def mean(data):\n",
        "    return sum(data) / len(data)\n",
        "\n",
        "def std_dev(data):\n",
        "    mu = mean(data)\n",
        "    return math.sqrt(sum([(x - mu)**2 for x in data]) / (len(data) - 1))\n",
        "\n",
        "def transpose(matrix):\n",
        "    return [[matrix[j][i] for j in range(len(matrix))] for i in range(len(matrix[0]))]\n",
        "\n",
        "def dot(v1, v2):\n",
        "    return sum(x*y for x, y in zip(v1, v2))\n",
        "\n",
        "def matrix_multiply(m1, m2):\n",
        "    m2_T = transpose(m2)\n",
        "    return [[dot(row1, col2) for col2 in m2_T] for row1 in m1]\n",
        "\n",
        "# --- 3. Core PCA Functions ---\n",
        "def standardize_data(X):\n",
        "    X_T = transpose(X)\n",
        "    means = [mean(col) for col in X_T]\n",
        "    stds = [std_dev(col) for col in X_T]\n",
        "\n",
        "    res = []\n",
        "    for row in X:\n",
        "        new_row = [(row[i] - means[i]) / stds[i] for i in range(len(row))]\n",
        "        res.append(new_row)\n",
        "    return res\n",
        "\n",
        "def get_covariance_matrix(X_std):\n",
        "    n_samples = len(X_std)\n",
        "    X_std_T = transpose(X_std)\n",
        "    return [[dot(col1, col2) / (n_samples - 1) for col2 in X_std_T] for col1 in X_std_T]\n",
        "\n",
        "def power_iteration(cov_matrix):\n",
        "    # Find the first principal component (eigenvector)\n",
        "    num_features = len(cov_matrix)\n",
        "    # Start with a random vector\n",
        "    b = [1.0] * num_features\n",
        "\n",
        "    for _ in range(100): # Iterate to converge\n",
        "        # Matrix-vector multiplication\n",
        "        b_next = [dot(row, b) for row in cov_matrix]\n",
        "        # Normalize\n",
        "        norm = math.sqrt(sum(x*x for x in b_next))\n",
        "        b = [x / norm for x in b_next]\n",
        "    return b\n",
        "\n",
        "def get_eigenvectors(cov_matrix, n_components):\n",
        "    eigenvectors = []\n",
        "    # Make a copy to modify\n",
        "    temp_cov = [row[:] for row in cov_matrix]\n",
        "\n",
        "    for _ in range(n_components):\n",
        "        # Find the eigenvector\n",
        "        vec = power_iteration(temp_cov)\n",
        "        eigenvectors.append(vec)\n",
        "\n",
        "        # Deflation: remove the found component from the matrix\n",
        "        eigenvalue = dot([dot(row, vec) for row in temp_cov], vec)\n",
        "        outer_product = [[v1*v2 for v2 in vec] for v1 in vec]\n",
        "\n",
        "        temp_cov = [\n",
        "            [temp_cov[i][j] - eigenvalue * outer_product[i][j] for j in range(len(vec))]\n",
        "            for i in range(len(vec))\n",
        "        ]\n",
        "    return eigenvectors\n",
        "\n",
        "def project_data(X_std, components):\n",
        "    projection_matrix = transpose(components)\n",
        "    return matrix_multiply(X_std, projection_matrix)\n",
        "\n",
        "\n",
        "# --- 4. Running PCA on the Iris Data ---\n",
        "# Standardize the dataset\n",
        "X_standardized = standardize_data(X)\n",
        "\n",
        "# Calculate the covariance matrix\n",
        "covariance_matrix = get_covariance_matrix(X_standardized)\n",
        "\n",
        "# Get the top 2 eigenvectors (principal components)\n",
        "top_2_components = get_eigenvectors(covariance_matrix, 2)\n",
        "\n",
        "# Project the data into 2D\n",
        "X_reduced = project_data(X_standardized, top_2_components)\n",
        "\n",
        "# Print the result\n",
        "print(\"Original number of features:\", len(X[0]))\n",
        "print(\"Reduced number of features:\", len(X_reduced[0]))\n",
        "print(\"\\nFirst 5 rows of the new 2D dataset:\")\n",
        "for row in X_reduced[:5]:\n",
        "    print([round(val, 4) for val in row])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuYB0hDhGiLh"
      },
      "source": [
        "Implement and demonstrate the FIND-S algorithm for finding the most specific hypothesis based on a given set of training data samples. Read the training data from a .CSV file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r14Q5dfwGlHl",
        "outputId": "26119ef2-ddc4-4cd6-81d3-28eddbd48041"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "enjoysport.csv file created successfully.\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "\n",
        "# The training data for the \"Enjoy Sport\" concept\n",
        "training_data = [\n",
        "    ['Sky', 'AirTemp', 'Humidity', 'Wind', 'Water', 'Forecast', 'EnjoySport'],\n",
        "    ['Sunny', 'Warm', 'Normal', 'Strong', 'Warm', 'Same', 'Yes'],\n",
        "    ['Sunny', 'Warm', 'High', 'Strong', 'Warm', 'Same', 'Yes'],\n",
        "    ['Rainy', 'Cold', 'High', 'Strong', 'Warm', 'Change', 'No'],\n",
        "    ['Sunny', 'Warm', 'High', 'Strong', 'Cool', 'Change', 'Yes']\n",
        "]\n",
        "\n",
        "# Write the data to a .csv file\n",
        "with open('enjoysport.csv', 'w', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(training_data)\n",
        "\n",
        "print(\"enjoysport.csv file created successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMnfoCUQG0Kh",
        "outputId": "8616f77f-8db9-442b-9cf3-edd01777cff0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Hypothesis: ['0', '0', '0', '0', '0', '0']\n",
            "\n",
            "--- Processing Example 1 ---\n",
            "Data: ['Sunny', 'Warm', 'Normal', 'Strong', 'Warm', 'Same'], Target: Yes\n",
            "Hypothesis updated: ['Sunny', 'Warm', 'Normal', 'Strong', 'Warm', 'Same']\n",
            "\n",
            "--- Processing Example 2 ---\n",
            "Data: ['Sunny', 'Warm', 'High', 'Strong', 'Warm', 'Same'], Target: Yes\n",
            "Hypothesis updated: ['Sunny', 'Warm', '?', 'Strong', 'Warm', 'Same']\n",
            "\n",
            "--- Processing Example 3 ---\n",
            "Data: ['Rainy', 'Cold', 'High', 'Strong', 'Warm', 'Change'], Target: No\n",
            "Negative example. Hypothesis remains unchanged.\n",
            "\n",
            "--- Processing Example 4 ---\n",
            "Data: ['Sunny', 'Warm', 'High', 'Strong', 'Cool', 'Change'], Target: Yes\n",
            "Hypothesis updated: ['Sunny', 'Warm', '?', 'Strong', '?', '?']\n",
            "\n",
            "-----------------------------------------\n",
            "The final, most specific hypothesis is: ['Sunny', 'Warm', '?', 'Strong', '?', '?']\n",
            "-----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "\n",
        "def find_s_algorithm(training_data):\n",
        "    \"\"\"\n",
        "    Implements the FIND-S algorithm from scratch.\n",
        "    \"\"\"\n",
        "    # Read the header to find the number of attributes\n",
        "    header = training_data[0]\n",
        "    num_attributes = len(header) - 1\n",
        "\n",
        "    # 1. Initialize hypothesis 'h' to the most specific possible hypothesis\n",
        "    # We use '0' to represent the null or empty value âˆ…\n",
        "    h = ['0'] * num_attributes\n",
        "    print(f\"Initial Hypothesis: {h}\\n\")\n",
        "\n",
        "    # Isolate the training examples (without the header)\n",
        "    examples = training_data[1:]\n",
        "\n",
        "    # 2. Iterate through each training example\n",
        "    for i, row in enumerate(examples):\n",
        "        # Separate attributes from the target concept\n",
        "        attributes = row[:-1]\n",
        "        target = row[-1]\n",
        "\n",
        "        print(f\"--- Processing Example {i+1} ---\")\n",
        "        print(f\"Data: {attributes}, Target: {target}\")\n",
        "\n",
        "        # 3. If the example is a positive instance\n",
        "        if target.lower() == 'yes':\n",
        "            for j in range(num_attributes):\n",
        "                # 4. Generalize the hypothesis\n",
        "                if h[j] == '0':\n",
        "                    # This is the first positive example\n",
        "                    h[j] = attributes[j]\n",
        "                elif h[j] != attributes[j]:\n",
        "                    # The attribute value is different, so generalize with '?'\n",
        "                    h[j] = '?'\n",
        "            print(f\"Hypothesis updated: {h}\\n\")\n",
        "        else:\n",
        "            # 5. If the example is negative, ignore it\n",
        "            print(\"Negative example. Hypothesis remains unchanged.\\n\")\n",
        "\n",
        "    return h\n",
        "\n",
        "# --- Main execution ---\n",
        "# Read the training data from the CSV file\n",
        "with open('enjoysport.csv', 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    data = list(reader)\n",
        "\n",
        "# Run the FIND-S algorithm\n",
        "final_hypothesis = find_s_algorithm(data)\n",
        "\n",
        "# Print the final result\n",
        "print(\"-----------------------------------------\")\n",
        "print(f\"The final, most specific hypothesis is: {final_hypothesis}\")\n",
        "print(\"-----------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxQttkKmG4AL"
      },
      "source": [
        "For a given set of training data examples stored in a .CSV file, implement and demonstrate the Candidate-Elimination algorithm to output a description of the set of all hypotheses consistent with the training examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I84S1tLeHHet",
        "outputId": "d0282d94-0c37-4825-8f4e-90fc306034db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "enjoysport.csv file created successfully.\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "\n",
        "# The training data for the \"Enjoy Sport\" concept\n",
        "training_data = [\n",
        "    ['Sky', 'AirTemp', 'Humidity', 'Wind', 'Water', 'Forecast', 'EnjoySport'],\n",
        "    ['Sunny', 'Warm', 'Normal', 'Strong', 'Warm', 'Same', 'Yes'],\n",
        "    ['Sunny', 'Warm', 'High', 'Strong', 'Warm', 'Same', 'Yes'],\n",
        "    ['Rainy', 'Cold', 'High', 'Strong', 'Warm', 'Change', 'No'],\n",
        "    ['Sunny', 'Warm', 'High', 'Strong', 'Cool', 'Change', 'Yes']\n",
        "]\n",
        "\n",
        "# Write the data to a .csv file\n",
        "with open('enjoysport.csv', 'w', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(training_data)\n",
        "\n",
        "print(\"enjoysport.csv file created successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMdxNTnBHJdz",
        "outputId": "7fa2171b-ee62-4578-cdff-bbc6b7ed1515"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial S: {('0', '0', '0', '0', '0', '0')}\n",
            "Initial G: {('?', '?', '?', '?', '?', '?')}\n",
            "------------------------------\n",
            "Processing Example 1: ['Sunny', 'Warm', 'Normal', 'Strong', 'Warm', 'Same', 'Yes']\n",
            "S1: {('Sunny', 'Warm', 'Normal', 'Strong', 'Warm', 'Same')}\n",
            "G1: {('?', '?', '?', '?', '?', '?')}\n",
            "------------------------------\n",
            "Processing Example 2: ['Sunny', 'Warm', 'High', 'Strong', 'Warm', 'Same', 'Yes']\n",
            "S2: {('Sunny', 'Warm', '?', 'Strong', 'Warm', 'Same')}\n",
            "G2: {('?', '?', '?', '?', '?', '?')}\n",
            "------------------------------\n",
            "Processing Example 3: ['Rainy', 'Cold', 'High', 'Strong', 'Warm', 'Change', 'No']\n",
            "S3: {('Sunny', 'Warm', '?', 'Strong', 'Warm', 'Same')}\n",
            "G3: {('?', 'Warm', '?', '?', '?', '?'), ('Sunny', '?', '?', '?', '?', '?'), ('?', '?', '?', '?', '?', 'Same')}\n",
            "------------------------------\n",
            "Processing Example 4: ['Sunny', 'Warm', 'High', 'Strong', 'Cool', 'Change', 'Yes']\n",
            "S4: {('Sunny', 'Warm', '?', 'Strong', '?', '?')}\n",
            "G4: {('?', 'Warm', '?', '?', '?', '?'), ('Sunny', '?', '?', '?', '?', '?')}\n",
            "------------------------------\n",
            "-----------------------------------------\n",
            "Final Specific Boundary (S): {('Sunny', 'Warm', '?', 'Strong', '?', '?')}\n",
            "Final General Boundary (G): {('?', 'Warm', '?', '?', '?', '?'), ('Sunny', '?', '?', '?', '?', '?')}\n",
            "-----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "\n",
        "def get_domains(examples):\n",
        "    \"\"\"Gets the unique attribute values for each column.\"\"\"\n",
        "    # Transpose the data to get columns\n",
        "    d = [list(set(col)) for col in zip(*examples)]\n",
        "    # We only care about the attributes, not the target concept\n",
        "    return d[:-1]\n",
        "\n",
        "def is_consistent(h, example):\n",
        "    \"\"\"Checks if a hypothesis is consistent with a single example.\"\"\"\n",
        "    # The last element is the target concept\n",
        "    if example[-1].lower() == 'no':\n",
        "        # If it's a negative example, h must NOT match\n",
        "        return not covers(h, example)\n",
        "    else:\n",
        "        # If it's a positive example, h MUST match\n",
        "        return covers(h, example)\n",
        "\n",
        "def covers(h, example):\n",
        "    \"\"\"Checks if a hypothesis covers (matches) an example.\"\"\"\n",
        "    for i in range(len(h)):\n",
        "        if h[i] != '?' and h[i] != example[i]:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def generalize_S(h, example):\n",
        "    \"\"\"Generalizes a hypothesis in S based on a positive example.\"\"\"\n",
        "    h_new = list(h)\n",
        "    for i in range(len(h)):\n",
        "        if h[i] == '0':\n",
        "            h_new[i] = example[i]\n",
        "        elif h[i] != example[i]:\n",
        "            h_new[i] = '?'\n",
        "    return tuple(h_new)\n",
        "\n",
        "def specialize_G(h, domains, example):\n",
        "    \"\"\"Specializes a hypothesis in G based on a negative example.\"\"\"\n",
        "    results = []\n",
        "    for i in range(len(h)):\n",
        "        if h[i] == '?':\n",
        "            for val in domains[i]:\n",
        "                if val != example[i]:\n",
        "                    h_new = list(h)\n",
        "                    h_new[i] = val\n",
        "                    results.append(tuple(h_new))\n",
        "    return results\n",
        "\n",
        "def candidate_elimination(examples):\n",
        "    domains = get_domains(examples)\n",
        "    num_attributes = len(domains)\n",
        "\n",
        "    # Initialize S and G\n",
        "    S = {tuple(['0'] * num_attributes)}\n",
        "    G = {tuple(['?'] * num_attributes)}\n",
        "\n",
        "    print(\"Initial S:\", S)\n",
        "    print(\"Initial G:\", G)\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    for i, example in enumerate(examples):\n",
        "        print(f\"Processing Example {i+1}: {example}\")\n",
        "        attributes = example[:-1]\n",
        "        target = example[-1]\n",
        "\n",
        "        if target.lower() == 'yes': # Positive Example\n",
        "            # 1. Remove inconsistent hypotheses from G\n",
        "            G = {g for g in G if covers(g, attributes)}\n",
        "\n",
        "            # 2. Generalize S\n",
        "            S_new = set()\n",
        "            for s in S:\n",
        "                if not covers(s, attributes):\n",
        "                    s_generalized = generalize_S(s, attributes)\n",
        "                    # Add to S_new only if it's consistent with G\n",
        "                    if any(all(g_i == '?' or g_i == s_g_i for g_i, s_g_i in zip(g, s_generalized)) for g in G):\n",
        "                        S_new.add(s_generalized)\n",
        "                else:\n",
        "                    S_new.add(s)\n",
        "            S = S_new\n",
        "\n",
        "        else: # Negative Example\n",
        "            # 1. Remove inconsistent hypotheses from S\n",
        "            S = {s for s in S if not covers(s, attributes)}\n",
        "\n",
        "            # 2. Specialize G\n",
        "            G_new = set()\n",
        "            for g in G:\n",
        "                if covers(g, attributes):\n",
        "                    for h in specialize_G(g, domains, attributes):\n",
        "                        # Add to G_new only if it's consistent with S\n",
        "                        if any(all(h_i == '?' or h_i == s_i for h_i, s_i in zip(h, s)) for s in S):\n",
        "                            G_new.add(h)\n",
        "                else:\n",
        "                    G_new.add(g)\n",
        "            G = G_new\n",
        "\n",
        "        # Prune S and G by removing overly general/specific hypotheses\n",
        "        S = {s for s in S if not any(all(s_i == '?' or s_i == s2_i for s_i, s2_i in zip(s, s2)) and s != s2 for s2 in S)}\n",
        "        G = {g for g in G if not any(all(g2_i == '?' or g2_i == g_i for g_i, g2_i in zip(g, g2)) and g != g2 for g2 in G)}\n",
        "\n",
        "        print(f\"S{i+1}: {S}\")\n",
        "        print(f\"G{i+1}: {G}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    return S, G\n",
        "\n",
        "# --- Main execution ---\n",
        "with open('enjoysport.csv', 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    data = list(reader)\n",
        "    # Exclude the header row for processing\n",
        "    training_examples = data[1:]\n",
        "\n",
        "final_S, final_G = candidate_elimination(training_examples)\n",
        "\n",
        "print(\"-----------------------------------------\")\n",
        "print(f\"Final Specific Boundary (S): {final_S}\")\n",
        "print(f\"Final General Boundary (G): {final_G}\")\n",
        "print(\"-----------------------------------------\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
